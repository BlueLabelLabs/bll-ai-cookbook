{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f005d0e",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "\n",
    "In this section, we'll import all the necessary libraries that our program will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f50b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pinecone\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30329141",
   "metadata": {},
   "source": [
    "# Loading Environment Variables\n",
    "\n",
    "Here, we'll load environment variables from a `.env` file. This file typically contains sensitive information like API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14133bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get OpenAI, Pinecone API keys from environment variables\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.environ.get(\"PINECONE_ENV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80684077",
   "metadata": {},
   "source": [
    "# Connecting to Pinecone\n",
    "\n",
    "We'll initialize the Pinecone connection using the API key and environment details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2726ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329af7f5",
   "metadata": {},
   "source": [
    "# Defining the Index Creation Function\n",
    "\n",
    "This function checks if an index exists in Pinecone. If it does and `force_recreate` is set to `True`, it deletes and recreates the index. Otherwise, it simply loads the existing index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_index(index_name, force_recreate=False):\n",
    "    if force_recreate and index_name in pinecone.list_indexes():\n",
    "        # If we need to force recreate, and the index already exists, delete it\n",
    "        pinecone.delete_index(index_name)\n",
    "\n",
    "    if index_name not in pinecone.list_indexes():\n",
    "        # Now we create the index if it doesn't exist (or if we just deleted it)\n",
    "        pinecone.create_index(\n",
    "            name=index_name,\n",
    "            metric='cosine',\n",
    "            dimension=1536\n",
    "        )\n",
    "        # Load and parse data\n",
    "        loader = DirectoryLoader(\n",
    "            './data', glob=\"**/*.txt\", loader_cls=TextLoader,\n",
    "            loader_kwargs={'autodetect_encoding': True, 'encoding': 'utf-8'}\n",
    "        )\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Split messages into chunks\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Initialize Pinecone vector store\n",
    "        vectorstore = Pinecone.from_documents(\n",
    "            docs, embeddings, index_name=index_name)\n",
    "    else:\n",
    "        # Initialize Pinecone vector store\n",
    "        vectorstore = Pinecone.from_existing_index(index_name, embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c8288",
   "metadata": {},
   "source": [
    "# Using the Index Creation Function\n",
    "\n",
    "We'll now call the function to either create or load the Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"whatsapp-demo\"\n",
    "vectorstore = create_or_load_index(index_name, force_recreate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ec0d4",
   "metadata": {},
   "source": [
    "# Querying the Vector Store\n",
    "\n",
    "We'll use a sample query to search for similar content in our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How you doing today?\"  # Replace with your query\n",
    "result = vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53b248",
   "metadata": {},
   "source": [
    "# Setting up the Chat Model and Prompt\n",
    "\n",
    "In this section, we'll set up the chat model and define the prompt for generating conversational responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9cc0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=.7)\n",
    "\n",
    "prompt_msgs = [\n",
    "    SystemMessage(\n",
    "        content=\"You are an AI assistant trying to simulate a conversation as Jose Cardama.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"You are given the following parts taken from a conversation. Provide a conversational response based on what Jose Cardama typically responds to:\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{context}\"),\n",
    "    HumanMessage(content=\"Tip: Do not answer more than what is asked.\"),\n",
    "    HumanMessage(content=\"Tip: If you don't know the answer, you can reply with 'Hmm, I'm not sure.'. Don't try to make up an answer.\"),\n",
    "    HumanMessage(\n",
    "        content=\"Message:\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "]\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=prompt_msgs, input_variables=[\"context\", \"input\"]\n",
    ")\n",
    "prompt.format(\n",
    "    context=result[0].page_content,\n",
    "    input=query,\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f7afc6",
   "metadata": {},
   "source": [
    "# Running the Chain Model\n",
    "\n",
    "Finally, we'll run the chain model to generate a response based on our query and the context from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.run({\"context\": result[0].page_content, \"input\": query}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
