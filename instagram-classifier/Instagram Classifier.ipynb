{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf671e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import PromptLayerChatOpenAI\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain, StuffDocumentsChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "import promptlayer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "gpt_model = \"gpt-4\"\n",
    "promptlayer.api_key = os.environ[\"PROMPTLAYER_API_KEY\"]\n",
    "openai = promptlayer.openai\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b414bc3",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326720b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"instagram-reel-archive.csv\"\n",
    "topic_filename = \"topics.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d93f3",
   "metadata": {},
   "source": [
    "# Read Data in From Instagram Reels CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1533b4",
   "metadata": {},
   "source": [
    "The Instagram posts that we are attempting to classify are contained within a CSV file. These will be read into an array of post captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename)\n",
    "captions = df['Caption'].iloc[1:]\n",
    "doc_captions = [Document(page_content=t) for t in captions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a17a2",
   "metadata": {},
   "source": [
    "# Read Existing Topics from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727f78a",
   "metadata": {},
   "source": [
    "This notebook is designed to taje as an input the global set of topics that have already been put together, most likely as an output from this notebook previously run. In this step we use Pandas to read the existing set of topics into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60316b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(topic_filename)\n",
    "existing_topics = df[\"topics\"]\n",
    "\n",
    "if len(existing_topics)!=1:\n",
    "    topics_combined = '\\n\\n'.join(existing_topics.astype(str))\n",
    "else:\n",
    "    topics_combined = ''\n",
    "    \n",
    "print(f\"Topics List: {topics_combined}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5428c427",
   "metadata": {},
   "source": [
    "# Map Prompt and Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd527f",
   "metadata": {},
   "source": [
    "The following prompt is used to develop the \"map\" step of the Map-Reduce chain. This prompt is run on each individual post, and is used to extract a set of \"topics\" local to that post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06761a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = PromptLayerChatOpenAI(model=gpt_model,pl_tags=[\"InstagramClassifier\"])\n",
    "map_template = \"\"\"The following is a set of captions taken from Instagram posts written by a Reproductive Endocrinologist, they are delimeted by ``` . \n",
    "\n",
    "Based on these captions please create a comprehensive list of topics relating to fertility, reproduction and women's health. \n",
    "\n",
    "If a post does not relate to any of those broad themes, please do not include them in the list of generated topics.\n",
    "```\n",
    "{captions}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm,prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03e35f",
   "metadata": {},
   "source": [
    "# Reduce Prompt and Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc49875",
   "metadata": {},
   "source": [
    "The following prompt is for the \"reduce\" step of the algorithm. It operates against the entire set of output that is produced by the \"map\" chain. In our example, the map chain yields a set of topics that are defined on the caption it was run on. These are then grouped together and passed as a result to this reduce chain. This reduce chain is designed to take this global output of the map step and reduce it down to a final set of unique fertility topics that minimized contextual overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853703b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_template = \"\"\"The following is a set of fertility, reproduction and women's health topics which are delimeted by ``` .\n",
    "Take these and organize these into a final, consolidated list of unique topics. \n",
    "\n",
    "You should combine topics into one that are similar but use different variations of the same words. \n",
    "\n",
    "Also, you should combine topics that use an acronym versus the full spelling. For example, 'TTC Myths' and 'Trying to Conceive Myths' should be a single topic 'Trying to Concieve Myths'.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "```\n",
    "{topics}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "reduce_prompt = PromptTemplate(template=reduce_template,input_variables=[\"topics\"],partial_variables={\"format_instructions\":format_instructions})\n",
    "#reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain=LLMChain(llm=llm,prompt=reduce_prompt)\n",
    "\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"topics\"\n",
    ")\n",
    "\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    token_max=4000)\n",
    "\n",
    "\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    llm_chain=map_chain,\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    document_variable_name=\"captions\",\n",
    "    return_intermediate_steps=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331fe4ab",
   "metadata": {},
   "source": [
    "# Run Map-Reduce Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66918194",
   "metadata": {},
   "source": [
    "In this step we run the actual MapReduceDocumentsChain, this will then begin to process all of the captions that were read in from the CSV file and perform the \"map\" step on each of them. Once the \"map\" step is completed opn all of them it will then run the final \"reduce\" step and output a final list of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d099831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = map_reduce_chain.run(doc_captions)\n",
    "new_topics = output_parser.parse(output)\n",
    "print(f\"Generated {len(new_topics)} new topics\")\n",
    "print(new_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc45041",
   "metadata": {},
   "source": [
    "# Merge New Topic List with Existing One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee84ce",
   "metadata": {},
   "source": [
    "In the following step, we use a new separate chain to eliminate any new topics which are already covered or similar to topics that were read in at the start of processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminate_duplicates_template = \"\"\"The items delimeted by ``` are a list of newly proposed reproductive health topics.\n",
    "\n",
    "The items delimeted by <> are a list of existing topics. \n",
    "\n",
    "Your job is to return a set of newly proposed topics that are different than any of the existing topics.\n",
    "\n",
    "You should treat any two topics that use different variations and conjugations of the same word roots as being the same.\n",
    "\n",
    "You should treat any two topics that use an acronym versus the full spelling of that acronym as being the same.\n",
    "\n",
    "For example, if a newly proposed topic is \"IVF Journey\", and there exists an existing topic called \"Fertility Journey\", these two topics are the same and you should not return \"IVF Journey\"\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "```\n",
    "{new_topics}\n",
    "```\n",
    "\n",
    "<{existing_topics}>\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(template=eliminate_duplicates_template, input_variables=[\"new_topics\",\"existing_topics\"],partial_variables={\"format_instructions\":format_instructions})\n",
    "llm_chain = LLMChain(llm=llm,prompt=prompt_template)\n",
    "output = llm_chain.predict(new_topics=output,existing_topics=topics_combined)\n",
    "new_topics = output_parser.parse(output)\n",
    "print(f\"Generated final set of {len(new_topics)} topics.\")\n",
    "print(new_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed392723",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92844c00",
   "metadata": {},
   "source": [
    "# Write back the new Topics List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c9292",
   "metadata": {},
   "source": [
    "In this step, we merge together the ready in topics list with the set of newly defined topics and write them back to the CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee00f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to combine the existing topics list with the new set of topics\n",
    "existing_topics_lower = [item.lower() for item in existing_topics]\n",
    "new_topics_lower = [item.lower() for item in new_topics]\n",
    "\n",
    "merged_topics_list_lower = existing_topics_lower + new_topics_lower\n",
    "unique_topics_list_lower = list(set(merged_topics_list_lower))\n",
    "\n",
    "final_topics_list = []\n",
    "for item in unique_topics_list_lower:\n",
    "    if item in existing_topics_lower:\n",
    "        final_topics_list.append(existing_topics[existing_topics_lower.index(item)])\n",
    "    elif item in new_topics_lower:\n",
    "        final_topics_list.append(new_topics[new_topics_lower.index(item)])\n",
    "\n",
    "print(final_topics_list)\n",
    "\n",
    "df = pd.DataFrame({'topics':final_topics_list})\n",
    "df.to_csv(topic_filename,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
